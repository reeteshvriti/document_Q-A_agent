# ðŸ“˜ Document Q&A Agent  

An intelligent question-answering system for PDF documents using **FastAPI, OpenAI embeddings, and Pinecone**.  
This project allows you to upload a PDF, automatically process its content into chunks, and then ask natural language questions about it. The system retrieves the most relevant chunks and generates accurate answers.  

---

##  Features  
-  Upload and process PDF documents via API  
-  Automatic text extraction, chunking, and metadata storage  
-  Semantic search powered by embeddings  
-  Ask natural language questions and receive context-aware answers  
-  Built with **FastAPI** for scalable backend APIs  
-  Ready for deployment on AWS / Azure  

---

##  Tech Stack  
- **Backend:** FastAPI, Python  
- **Document Processing:** pdfplumber, PyPDF2  
- **Embeddings & LLM:** OpenAI API  
- **Vector Database:** Pinecone (for storing & retrieving embeddings)  
- **Deployment (future):** AWS, Azure  



## Project Structure  

document_Q-A_agent/
â”‚â”€â”€ .env                        # API keys (OpenAI, DB credentials, etc.)
â”‚â”€â”€ .gitignore                  # Ignore .env, __pycache__, etc.
â”‚â”€â”€ requirements.txt            # Python dependencies
â”‚â”€â”€ README.md                   # Project documentation
â”‚â”€â”€ dockerfile                  # (Optional) for containerization
â”‚â”€â”€ config.yaml                 # (Optional) central config (chunk size, top_k)
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ingestion.py        # Endpoint for PDF upload & processing
â”‚   â”‚   â”œâ”€â”€ query.py            # Endpoint for user queries
â”‚   â”‚   â””â”€â”€ chat_history.py     # Endpoint for retrieving/storing chat history
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chunking.py         # Hybrid recursive character splitter
â”‚   â”‚   â”œâ”€â”€ embeddings.py       # OpenAI embeddings logic
â”‚   â”‚   â”œâ”€â”€ vectordb.py         # ChromaDB wrapper
â”‚   â”‚   â”œâ”€â”€ llm_chain.py        # LangChain ConversationalRetrievalChain setup
â”‚   â”‚   â””â”€â”€ utils.py            # Logging, helpers
â”‚   â”‚
â”‚   â””â”€â”€ main.py                 # FastAPI entrypoint
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                  # Streamlit UI
â”‚   â””â”€â”€ components/             # Custom UI widgets if needed
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploaded_pdfs/          # Temporary storage of PDFs
â”‚   â””â”€â”€ chromadb/               # Local vector DB persistence
â”‚
â””â”€â”€ db/
    â”œâ”€â”€ chat_history.db         # SQLite database (local dev)
    â””â”€â”€ migrations/             # (If you later move to Postgres)




---


Future enhancements

1. storing chat history 
2. Track API usage & cost monitoring via LangServe
3. User authentication & access control


### System Design

![System Design](results_sec/syatem_design.jpeg)

##  Demo

### Main screen
![Main Screen](results_sec/main_screen.png)

### Upload docx
![Upload Docx ](results_sec/doc_uploaded.png)

### Ask Question
![Q&A Demo](results_sec/ask_endpoint.png)

### Agent response
![Agent response](results_sec/agent_response.png)


